from langchain.prompts.chat import HumanMessagePromptTemplate
from langchain.output_parsers import StructuredOutputParser
from langchain_google_vertexai import HarmBlockThreshold 
from langchain.prompts.chat import ChatPromptTemplate
from langchain.output_parsers import ResponseSchema
from langchain_google_vertexai import HarmCategory
from langchain_google_vertexai import ChatVertexAI
from langchain.prompts import PromptTemplate
from src.config.logging import logger
from src.config.setup import config
from typing import Optional


safety_settings = {
    HarmCategory.HARM_CATEGORY_UNSPECIFIED: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE
}

class LLM:
    """
    A class representing a Language Model using Vertex AI.

    Attributes:
        model (ChatVertexAI): The chat model loaded from Vertex AI.
    """

    def __init__(self) -> None:
        """
        Initializes the LLM class by loading the chat model.
        """
        self.model = self._initialize_model()

    def _initialize_model(self) -> Optional[ChatVertexAI]:
        """
        Loads the chat model from Vertex AI.

        Returns:
            ChatVertexAI: An instance of the Vertex AI chat model.
        """
        try:
            model = ChatVertexAI(
                model_name=config.TEXT_GEN_MODEL_NAME,
                temperature=0.0,
                top_k=1.0,
                top_p=0.0,
                max_output_tokens=4096,
                verbose=True, 
                safety_settings=safety_settings)
            logger.info("Chat model loaded successfully.")
            return model
        except Exception as e:
            logger.error(f"Failed to load the model: {e}")
            return None

    def predict(self, task: str, query: str) -> Optional[str]:
        """
        Generates a response for a given task and query using the chat model.

        Args:
            task (str): The task to be performed by the model.
            query (str): The query or input text for the model.

        Returns:
            Optional[str]: The model's response or None if an error occurred.
        """
        try:
            human_template = "{task}\nQuery:\n{query}"
            human_message = HumanMessagePromptTemplate.from_template(human_template)
            chat_template = ChatPromptTemplate.from_messages([human_message])
            prompt = chat_template.format_prompt(task=task, query=query).to_messages()
            response = self.model.invoke(prompt)
            completion = response.content
            return completion
        except Exception as e:
            logger.error(f"Error during model prediction: {e}")
            return None
        

    def compare(self, task: str, expected_ans: str, predicted_ans: str) -> dict:
        """
        Compares an expected answer with a predicted answer for a given task, evaluating
        factual correctness and generating an explanation.

        Args:
            task (str): The task or context for the answers.
            expected_ans (str): The expected or reference answer.
            predicted_ans (str): The answer generated by the model.

        Returns:
            dict: A dictionary with the following fields:
                * class (str): "correct", "incorrect" or "Partially Correct" based on the comparison.
                * rationale (str): An explanation for the classification, highlighting 
                                differences and potential factual inaccuracies.
        """
        try:
            # Structured Output Setup
            response_schemas = [
                ResponseSchema(name="class", description="Whether the predicted answer is 'correct', 'incorrect' or 'partially correct."),
                ResponseSchema(name="rationale", description="Explanation for why the answer is incorrect or partially correct , with specific details."),
            ]
            output_parser = StructuredOutputParser.from_response_schemas(response_schemas)
            format_instructions = output_parser.get_format_instructions()
            
            # Prompt Template
            template = """
            Task: {task}

            Expected Answer: {expected_ans}

            Predicted Answer: {predicted_ans}

            Compare the predicted answer with the expected answer. Determine if the predicted answer is factually correct.

            Provide your response in the following format:

            {format_instructions}
            """

            prompt = PromptTemplate(
                input_variables=["task", "expected_ans", "predicted_ans"],
                template=template,
                partial_variables={"format_instructions": format_instructions},
            )

            prompt_msg = prompt.format_prompt(task=task, expected_ans=expected_ans, predicted_ans=predicted_ans).to_messages()
            response = self.model.invoke(prompt_msg)
            result_dict = output_parser.parse(response.content)
            return result_dict
        except Exception as e:
            logger.error(f"Error during comparison: {e}")
            return {"class": "error", "rationale": "An error occurred during the comparison process."}